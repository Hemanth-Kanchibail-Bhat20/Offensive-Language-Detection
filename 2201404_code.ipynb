{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rv6IUyt-6gOz"
      },
      "outputs": [],
      "source": [
        "# Registration number :- 2201404"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvFbA9Q-qorm"
      },
      "source": [
        "Installing and importing all the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5bj1joxqiuN",
        "outputId": "322ae10c-384d-4d14-d0b1-d4b8337dae97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622380 sha256=c2534d48f9a47845479abe72babf5052711a9288193356a3fdb431596642fa05\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/0f/23/3c010c3fd877b962146e7765f9e9b08026cac8b035094c5750\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install autocorrect \n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split # To split the data into 25%, 50%, 75%, 100%.\n",
        "import nltk # Natural Language Toolkit library is used widely in text classification tasks.\n",
        "# Libraries used for data preprocessing.\n",
        "from nltk.tokenize import word_tokenize # Tokenization\n",
        "from nltk.stem import WordNetLemmatizer # Lemmatization\n",
        "import re\n",
        "import html\n",
        "from nltk.corpus import stopwords\n",
        "from autocorrect import Speller\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from joblib import dump, load\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "# Download the 'wordnet' resource\n",
        "nltk.download('wordnet')\n",
        "# Download the 'stopwords' resource\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlNTQHJzr2Ak"
      },
      "source": [
        "Set student ID as a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eeHSElo9qxft"
      },
      "outputs": [],
      "source": [
        "student_id = 2201404"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4iyibTar-9H"
      },
      "source": [
        "Set seed for all libraries as my student id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jyxYI0SRsIAd"
      },
      "outputs": [],
      "source": [
        "# numpy seed\n",
        "np.random.seed(student_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wZ5fk4Ltfbz"
      },
      "source": [
        "# **Load the dataset**\n",
        "\n",
        "---\n",
        "\n",
        "In this section, I will access all the files from the Google Drive, create the subsets of training data [25,50,75,100] as per the guideline and save the files in the gdrive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouAPFeCStMq3"
      },
      "source": [
        "Connecting to Gdrive to access datasets and perform operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdJSWyrNtQO7",
        "outputId": "6adc05a2-7db8-4d25-a500-b13dad1857cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "List files:  ['valid.csv', 'train.csv', 'test.csv', 'models']\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Accessing the files in the Gdrive\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(student_id)) # student_id = 2201404, represents the folder where all files are stored.\n",
        "GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcVIfy2PH8EM"
      },
      "source": [
        "Accessing model -1 directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MdK5Ir3b-Rj",
        "outputId": "17f7f112-6aa7-4e03-fc09-33fcde8e2ee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 1 directory:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/1\n",
            "Model 1 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/25\n",
            "Model 1 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/50\n",
            "Model 1 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/75\n",
            "Model 1 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/100\n"
          ]
        }
      ],
      "source": [
        "MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1') # Model 1 directory\n",
        "print('Model 1 directory: ', MODEL_1_DIRECTORY)\n",
        "\n",
        "MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'25') # Model 1 trained using 25% of train data directory\n",
        "print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)\n",
        "\n",
        "MODEL_1_50_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'50') # Model 1 trained using 50% of train data directory\n",
        "print('Model 1 directory with 50% data: ', MODEL_1_50_DIRECTORY)\n",
        "\n",
        "MODEL_1_75_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'75') # Model 1 trained using 75% of train data directory\n",
        "print('Model 1 directory with 75% data: ', MODEL_1_75_DIRECTORY)\n",
        "\n",
        "MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY,'100') # Model 1 trained using 100% of train data directory\n",
        "print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-KFtBPBIC8_"
      },
      "source": [
        "Accessing model-2 directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSdbIFmOIFmP",
        "outputId": "e04e6280-12ff-43ed-dbe1-2f3e2dd77ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 2 directory:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/2\n",
            "Model 2 directory with 25% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/25\n",
            "Model 2 directory with 50% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/50\n",
            "Model 2 directory with 75% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/75\n",
            "Model 2 directory with 100% data:  gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/100\n"
          ]
        }
      ],
      "source": [
        "MODEL_2_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '2') # Model 2 directory\n",
        "print('Model 2 directory: ', MODEL_2_DIRECTORY)\n",
        "\n",
        "MODEL_2_25_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'25') # Model 2 trained using 25% of train data directory\n",
        "print('Model 2 directory with 25% data: ', MODEL_2_25_DIRECTORY)\n",
        "\n",
        "MODEL_2_50_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'50') # Model 2 trained using 50% of train data directory\n",
        "print('Model 2 directory with 50% data: ', MODEL_2_50_DIRECTORY)\n",
        "\n",
        "MODEL_2_75_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'75') # Model 2 trained using 75% of train data directory\n",
        "print('Model 2 directory with 75% data: ', MODEL_2_75_DIRECTORY)\n",
        "\n",
        "MODEL_2_100_DIRECTORY = os.path.join(MODEL_2_DIRECTORY,'100') # Model 2 trained using 100% of train data directory\n",
        "print('Model 2 directory with 100% data: ', MODEL_2_100_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKF2RRUR9_DV"
      },
      "source": [
        "Accessing the train.csv, Valid.csv and test.csv files in google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avBrPY6O0DrN",
        "outputId": "8b73c71b-0a7a-46ed-949c-36f6c05f1ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train file:  gdrive/MyDrive/./CE807/Assignment2/2201404/train.csv\n",
            "Training data\n",
            "       id                                              tweet label\n",
            "0  42884  @USER I’m done with you as well. An INTENTIONA...   NOT\n",
            "1  92152  I now have over 6k followers.  Only 94k to go ...   NOT\n",
            "2  65475  @USER Tom was bought! He is more interested in...   NOT\n",
            "3  22144  @USER @USER Even her brother thinks she is a m...   OFF\n",
            "4  81048  @USER @USER @USER @USER @USER I can understand...   OFF\n",
            "\n",
            "valid file:  gdrive/MyDrive/./CE807/Assignment2/2201404/valid.csv\n",
            "validation data\n",
            "       id                                              tweet label\n",
            "0  12476  @USER @USER @USER Trump is declassifying infor...   NOT\n",
            "1  23242  @USER Ha even with them trying to rig the syst...   NOT\n",
            "2  97885  @USER @USER @USER Man you really thought this ...   NOT\n",
            "3  43414  @USER Ms. Clinton - you are a class act. My re...   OFF\n",
            "4  81403  @USER It’s still here at 753. If someone did t...   NOT\n",
            "\n",
            "test file:  gdrive/MyDrive/./CE807/Assignment2/2201404/test.csv\n",
            "Testing data\n",
            "       id                                              tweet label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF\n"
          ]
        }
      ],
      "source": [
        "# Accessing the train.csv file in google drive.\n",
        "train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')\n",
        "print('\\nTrain file: ', train_file)\n",
        "\n",
        "train_data = pd.read_csv(train_file)\n",
        "print(\"Training data\\n\", train_data.head())\n",
        "\n",
        "# Accessing the valid.csv file in google drive.\n",
        "valid_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')\n",
        "print('\\nvalid file: ', valid_file)\n",
        "\n",
        "valid_data = pd.read_csv(valid_file)\n",
        "print(\"validation data\\n\", valid_data.head())\n",
        "\n",
        "# Accessing the test.csv file in google drive.\n",
        "test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')\n",
        "print('\\ntest file: ', test_file)\n",
        "\n",
        "test_data = pd.read_csv(test_file)\n",
        "print(\"Testing data\\n\", test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aabclwb3B1MI"
      },
      "source": [
        "# **Data Preprocessing**\n",
        "\n",
        "---\n",
        "\n",
        "Data preprocessing is an important step in preparing the text data for analysis and improving the performance of the classification models. In this section, I will perform \n",
        "\n",
        "*   **Text Normalization**:- In order to ensure that words are consistently represented in the text data, convert the text to lowercase. By reducing the amount of variation in word forms, text normalization makes it simpler for machine learning models to find patterns and characteristics in text data.\n",
        "\n",
        "*  **Removing HTML tags and URLs**:- Remove HTML tags because text data often contains HTML tags that are not relevant to the analysis and may introduce noise or inconsistencies in the data. HTML tags are used to format and structure web pages and may contain various types of content, such as links, images, or formatting instructions, which are not meaningful in the context of offensive language detection.\n",
        "\n",
        "*   **Remove special characters and digits**:- because these characters are often considered noise or irrelevant in the analysis of offensive language.\n",
        "\n",
        "*   **Tokenize the text**:- Tokenization allows for the analysis of the text at a granular level. By breaking text into tokens, it becomes possible to count occurrences of specific words, phrases, or patterns, and analyze their frequency, distribution, and relationships. This can provide valuable insights into the language used in offensive text and help in identifying offensive patterns or patterns indicative of offensive language.\n",
        "\n",
        "*   **Remove stopwords**:- Stop words are commonly occurring words in english language, such as \"a\", \"an\", \"the\", \"is\", \"in\", etc., that do not carry much meaning on their own and are often considered noise in text data. This provides Noise reduction, Reduced dimentionality and more focus on meaningfull words helping to better analyze text and improve the performance of classification models.\n",
        "\n",
        "*   **Correct misspellings and typos**:- Correcting misspellings and typos can help improve the quality and reliability of the text data, leading to more accurate results, enhanced clarity and consistency.\n",
        "\n",
        "\n",
        "*   **Lemmatize the tokens**:- Lemmatization is a text preprocessing technique that aims to reduce words to their base or root form, known as a lemma.\n",
        "\n",
        "*   **Join tokens back into a preprocessed text**:- is an important step as text data is typically represented as a sequence of tokens during text preprocessing. Joining these tokens back into a preprocessed text allows for the original context to be reconstructed in a format that can be more easily understood and analyzed. This allows the preprocessed data to be fed as input to these models in a format that they can effectively process.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILTWKn4HCB4L"
      },
      "source": [
        "Now that we have access to the data files ready to use, Let's preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uOltLXRP7KLq"
      },
      "outputs": [],
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize spell checker\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(['@user', 'url', 'user', 'k'] + stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Correct misspellings and typos\n",
        "    tokens = [spell(word) for word in tokens]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into a preprocessed text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZkkD_ADX_jY"
      },
      "source": [
        "Performing data Preprocessing on all datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCsBd4I4MATK",
        "outputId": "86a5588f-c2d6-4218-e4e1-768a86ade4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      id                                              tweet label\n",
            "0  42884  done well intentional th hour attack completel...   NOT\n",
            "1  92152           follower go get goal trumptrain manga kg   NOT\n",
            "2  65475              tom bought interested tom constituent   NOT\n",
            "3  22144                         even brother think monster   OFF\n",
            "4  81048  understand someone know debate willing spend m...   OFF \n",
            "Training data preprocessing completed\n",
            "\n",
            "      id                                              tweet label\n",
            "0  12476  trump classifying information nothing subject ...   NOT\n",
            "1  23242  ha even trying rig system idea divine interven...   NOT\n",
            "2  97885                             man really thought huh   NOT\n",
            "3  43414  m clinton class act response idiot would likel...   OFF\n",
            "4  81403             still someone family enraged let loose   NOT \n",
            "Validation data preprocessing completed\n",
            "\n",
            "      id                                              tweet label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF \n",
            "Testing data preprocessing completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Applying the preprocess_text function on all datasets that were accessed from the Gdrive.\n",
        "\n",
        "# Apply the function to 'tweet' column of the original training data.\n",
        "train_data['tweet'] = train_data['tweet'].apply(preprocess_text)\n",
        "print(train_data.head(), \"\\nTraining data preprocessing completed\\n\")\n",
        "\n",
        "# Apply the function to 'tweet' column of the validation data.\n",
        "valid_data['tweet'] = valid_data['tweet'].apply(preprocess_text)\n",
        "print(valid_data.head(), \"\\nValidation data preprocessing completed\\n\")\n",
        "\n",
        "# Apply the function to 'tweet' column of the testing data.\n",
        "train_data['tweet'] = train_data['tweet'].apply(preprocess_text)\n",
        "print(test_data.head(), \"\\nTesting data preprocessing completed\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT1SpWnka2oM"
      },
      "source": [
        "Checking the dimension of the Trainig, Validation and Testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfcmInn3a8ai",
        "outputId": "75fa8b36-77e2-416a-ed1b-92db8121d1b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimension of the training dataset:  (12313, 3)\n",
            "Dimension of the Validation dataset:  (927, 3)\n",
            "Dimension of the testing dataset:  (860, 3)\n"
          ]
        }
      ],
      "source": [
        "print(\"Dimension of the training dataset: \", train_data.shape)\n",
        "print(\"Dimension of the Validation dataset: \", valid_data.shape)\n",
        "print(\"Dimension of the testing dataset: \", test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK7VQD2rbXdK"
      },
      "source": [
        "Checking the imbalance in dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0IeKJaObcEj",
        "outputId": "20398f51-7b34-434c-af0f-583f687d1f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data contains: NOT    8221\n",
            "OFF    4092\n",
            "Name: label, dtype: int64\n",
            "Validation data contains: NOT    619\n",
            "OFF    308\n",
            "Name: label, dtype: int64\n",
            "Testing data contains: NOT    620\n",
            "OFF    240\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check the distribution of classes in the training dataset\n",
        "train_class_counts = train_data['label'].value_counts() \n",
        "print(\"Training data contains:\", train_class_counts)\n",
        "\n",
        "# Check the distribution of classes in the validation dataset\n",
        "valid_class_counts = valid_data['label'].value_counts()  \n",
        "print(\"Validation data contains:\", valid_class_counts)\n",
        "\n",
        "# Check the distribution of classes in the test dataset\n",
        "test_class_counts = test_data['label'].value_counts() \n",
        "print(\"Testing data contains:\", test_class_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIiogEKbxpfG"
      },
      "source": [
        "# **Split the training data as per the guidelines and save it into the Gdrive.**\n",
        "\n",
        "---\n",
        "\n",
        "*   train_25 = 25% of training data.\n",
        "*   train_50 = train_25 + 25% of training data (50% of training data).\n",
        "*   train_75 = train_50 + 25% of training data (75% of training data)\n",
        "*   train_100 = train_75 + remaining 25% of training data (100% of training data).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzknrYrd2ntS"
      },
      "source": [
        "In the below code, \n",
        "*   train_25 data subset has 25% of training data.\n",
        "*   To create train_50 subset (add 25% to the existing train_25 data) I have selected remaining 75% data by removing the rows (samples) from the training data that correspond to the indices obtained from train_25.index, which gives us the remaining data (i.e., the data not included in train_25 subset) and taken 25% data from 75% data to make sure data is not repeated. \n",
        "*   train_75 subset is created using the same method as train_50 (adding 25% of from the remaining 50% of traininig data to train_50).\n",
        "*   train_100 subset is 100% of data (full dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mHeP2mDguIT4"
      },
      "outputs": [],
      "source": [
        "# Set the seed for reproducibility\n",
        "np.random.seed(student_id)\n",
        "\n",
        "# The \"_\" (underscore) is used as a placeholder variable to discard the second return value from the train_test_split() function. \n",
        "# In this case, it is used to discard the test data that is generated by the function, \n",
        "# and only keep the extra data that is sampled as additional 25% training data from the remaining train_data.\n",
        "\n",
        "# The stratify parameter is used to ensure that the class distribution in the original train_data is maintained in the generated train data subsets.  \n",
        "\n",
        "# Split train_data into train_25 (25% of data)\n",
        "train_25, _ = train_test_split(train_data, train_size=0.25, stratify=train_data['label'], random_state=1)\n",
        "\n",
        "# Add 25% more data to train_25 to create train_50 (50% of data)\n",
        "add_25_data, _ = train_test_split(train_data.drop(train_25.index), train_size=0.25, stratify=train_data.drop(train_25.index)['label'])\n",
        "train_50 = pd.concat([train_25, add_25_data])\n",
        "\n",
        "# Add 25% more data to train_50 to create train_75 (75% of data)\n",
        "add_50_data, _ = train_test_split(train_data.drop(train_50.index), train_size=0.25, stratify=train_data.drop(train_50.index)['label'])\n",
        "train_75 = pd.concat([train_50, add_50_data])\n",
        "\n",
        "# The remaining data is train_100 (100% of data)\n",
        "train_100 = train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNG5eLWk4_NQ"
      },
      "source": [
        "Converting all the subsets of data into csv files and saving them into the Gdrive. All the files are stored in the directory with dir_name as my student_is (2201404)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6aEKB495Iyw",
        "outputId": "2bdbd289-838d-48cb-bb17-2e8bc55f07f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset save successfull\n"
          ]
        }
      ],
      "source": [
        "# Convert all training data subsets into csv files and save them into the google drive.\n",
        "# index = False is a method  of pandas library that is used to exclude the index column from being saved in the CSV file.\n",
        "\n",
        "# Location where the data needs to be saved\n",
        "# Define save_directory\n",
        "save_directory = os.path.join(GOOGLE_DRIVE_PATH, str(student_id))\n",
        "\n",
        "train_25.to_csv(save_directory + '_train_25.csv', index=False)\n",
        "train_50.to_csv(save_directory + '_train_50.csv', index=False)\n",
        "train_75.to_csv(save_directory + '_train_75.csv', index=False)\n",
        "train_100.to_csv(save_directory + '_train_100.csv', index=False)\n",
        "\n",
        "print(\"Dataset save successfull\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtYH6u39-fu9"
      },
      "source": [
        "Accessing the csv files in the Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EbaxG72MqW0",
        "outputId": "d0a75651-12b4-4c4b-dd29-94af6262dccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "train_25 file:  gdrive/MyDrive/./CE807/Assignment2/2201404/2201404_train_25.csv\n",
            "train_25 data\n",
            "       id                                              tweet label\n",
            "0  20145                                            deserve   NOT\n",
            "1  27155  say one one follower following two stand origi...   NOT\n",
            "2  62109                 america succeeding president trump   NOT\n",
            "3  14809  woman accusing supreme court nominee kavanaugh...   NOT\n",
            "4  18747  amazing admire much always able raise holiness...   NOT\n",
            "\n",
            "train_50 file:  gdrive/MyDrive/./CE807/Assignment2/2201404/2201404_train_50.csv\n",
            "train_50 data\n",
            "       id                                              tweet label\n",
            "0  20145                                            deserve   NOT\n",
            "1  27155  say one one follower following two stand origi...   NOT\n",
            "2  62109                 america succeeding president trump   NOT\n",
            "3  14809  woman accusing supreme court nominee kavanaugh...   NOT\n",
            "4  18747  amazing admire much always able raise holiness...   NOT\n",
            "\n",
            "train_75 file:  gdrive/MyDrive/./CE807/Assignment2/2201404/2201404_train_75.csv\n",
            "train_75 data\n",
            "       id                                              tweet label\n",
            "0  20145                                            deserve   NOT\n",
            "1  27155  say one one follower following two stand origi...   NOT\n",
            "2  62109                 america succeeding president trump   NOT\n",
            "3  14809  woman accusing supreme court nominee kavanaugh...   NOT\n",
            "4  18747  amazing admire much always able raise holiness...   NOT\n",
            "\n",
            "train_100 file:  gdrive/MyDrive/./CE807/Assignment2/2201404/2201404_train_100.csv\n",
            "train_100 data\n",
            "       id                                              tweet label\n",
            "0  42884  done well intentional th hour attack completel...   NOT\n",
            "1  92152           follower go get goal trumptrain manga kg   NOT\n",
            "2  65475              tom bought interested tom constituent   NOT\n",
            "3  22144                         even brother think monster   OFF\n",
            "4  81048  understand someone know debate willing spend m...   OFF\n"
          ]
        }
      ],
      "source": [
        "# Original Training data, Validation data and testing data access is already done above. \n",
        "\n",
        "# Accessing the train_25.csv file in google drive.\n",
        "train_25_file = os.path.join(GOOGLE_DRIVE_PATH, '2201404_train_25.csv')\n",
        "print('\\ntrain_25 file: ', train_25_file)\n",
        "\n",
        "train_25_data = pd.read_csv(train_25_file)\n",
        "print(\"train_25 data\\n\", train_25_data.head())\n",
        "\n",
        "# Accessing the train_50.csv file in google drive.\n",
        "train_50_file = os.path.join(GOOGLE_DRIVE_PATH, '2201404_train_50.csv')\n",
        "print('\\ntrain_50 file: ', train_50_file)\n",
        "\n",
        "train_50_data = pd.read_csv(train_50_file)\n",
        "print(\"train_50 data\\n\", train_50_data.head())\n",
        "\n",
        "# Accessing the train_75.csv file in google drive.\n",
        "train_75_file = os.path.join(GOOGLE_DRIVE_PATH, '2201404_train_75.csv')\n",
        "print('\\ntrain_75 file: ', train_75_file)\n",
        "\n",
        "train_75_data = pd.read_csv(train_75_file)\n",
        "print(\"train_75 data\\n\", train_75_data.head())\n",
        "\n",
        "# Accessing the train_100.csv file in google drive.\n",
        "train_100_file = os.path.join(GOOGLE_DRIVE_PATH, '2201404_train_100.csv')\n",
        "print('\\ntrain_100 file: ', train_100_file)\n",
        "\n",
        "train_100_data = pd.read_csv(train_100_file)\n",
        "print(\"train_100 data\\n\", train_100_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utiZLbEQ8j8t"
      },
      "source": [
        "Checking the imbalance in dataset in the Train_25, Train_50, Train_75 and Train_100 subsets of training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yTUwsPC8owO",
        "outputId": "4d48969d-a0d4-4ad5-d826-3e15bee338de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train_25 data contains: NOT    2055\n",
            "OFF    1023\n",
            "Name: label, dtype: int64\n",
            "Train_50 data contains: NOT    3596\n",
            "OFF    1790\n",
            "Name: label, dtype: int64\n",
            "Train_75 data contains: NOT    4752\n",
            "OFF    2365\n",
            "Name: label, dtype: int64\n",
            "TTrain_100 data contains: NOT    8221\n",
            "OFF    4092\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check the distribution of classes in the train_25 dataset\n",
        "train_25_class_counts = train_25_data['label'].value_counts()  \n",
        "print(\"Train_25 data contains:\", train_25_class_counts)\n",
        "\n",
        "# Check the distribution of classes in the train_50 dataset\n",
        "train_50_class_counts = train_50_data['label'].value_counts()  \n",
        "print(\"Train_50 data contains:\", train_50_class_counts)\n",
        "\n",
        "# Check the distribution of classes in the train_75 dataset\n",
        "train_75_class_counts = train_75_data['label'].value_counts()  \n",
        "print(\"Train_75 data contains:\", train_75_class_counts)\n",
        "\n",
        "# Check the distribution of classes in the train_100 dataset\n",
        "train_100_class_counts = train_100_data['label'].value_counts()  \n",
        "print(\"TTrain_100 data contains:\", train_100_class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLmgD30-b-nI"
      },
      "source": [
        "## Model **Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5TPohYcFDf8"
      },
      "source": [
        "Common Codes\n",
        "\n",
        "\n",
        "*   Prepare dataset as required to train and test the model.\n",
        "*   Evaluate model performance\n",
        "\n",
        "*   Save model and count vectorizer as pickel in GDrive\n",
        "*   Load model and count vectorizer as pickel from GDrive\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDcZ51goFd_0"
      },
      "source": [
        "Function to vectorizing the data, see the difference training and other cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sYi8dl3oFFL-"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(df, vectorizer=None, split='test'):\n",
        "    \"\"\"\n",
        "     Takes input as pandas DataFrame, and returns the processed data as a list of strings.\n",
        "     Also returns the trained CountVectorizer object if not provided.\n",
        "    Args:\n",
        "        df: DataFrame containing the text data\n",
        "        vectorizer: CountVectorizer object, if already trained\n",
        "        split: String specifying the split type, 'train' or 'test'\n",
        "    Returns:\n",
        "        values: List of strings containing the processed data\n",
        "        vectorizer: CountVectorizer object\n",
        "    \"\"\"\n",
        "    # Replace NaN values with empty string\n",
        "    df['tweet'].fillna('', inplace=True)\n",
        "    \n",
        "    if vectorizer is None:\n",
        "        vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
        "        vectorizer.fit(df['tweet'])\n",
        "    values = vectorizer.transform(df['tweet'])\n",
        "    print(f'{split} data prepared with {values.shape[0]} datapoints.')\n",
        "    return values, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdv0rXTuFlvD"
      },
      "source": [
        "Evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fh_Z0yHuFpk8"
      },
      "outputs": [],
      "source": [
        "def compute_performance(y_true, y_pred, split='test'):\n",
        "    \"\"\"\n",
        "     Takes input as true labels and predicted labels, and computes multiple performance metrics.\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        split: String specifying the split type, 'train' or 'test'\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Compute performance metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    \n",
        "    # Print performance metrics\n",
        "    print(f'{split} Accuracy: {accuracy:.4f}')\n",
        "    print(f'{split} F1-score (macro): {f1:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMdpZvd_FqbE"
      },
      "source": [
        "Save model and count vectorizer as pickel in GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XJy7zQvSFvdc"
      },
      "outputs": [],
      "source": [
        "# Save the model and vectorizer\n",
        "def save_model(model, vectorizer, model_dir):\n",
        "    model_file = os.path.join(model_dir, 'model.sav')\n",
        "    vectorizer_file = os.path.join(model_dir, 'vectorizer.sav')\n",
        "    dump(model, model_file)\n",
        "    dump(vectorizer, vectorizer_file)\n",
        "    print('Model and vectorizer saved successfully.')\n",
        "    return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ELP5BobFxqF"
      },
      "source": [
        "Load model and count vectorizer as pickel from GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "WO6gDgbOF055"
      },
      "outputs": [],
      "source": [
        "# Load the model and vectorizer\n",
        "def load_model(model_file, vectorizer_file):\n",
        "    model = load(model_file)\n",
        "    vectorizer = load(vectorizer_file)\n",
        "    print('Model and vectorizer loaded successfully.')\n",
        "    return model, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2NDZDJ2Eg9u"
      },
      "source": [
        "## Model -1 (Random Forest Classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpqaHO-vF78t"
      },
      "source": [
        "Model -1 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HkTlXjEvGFfE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model1(X, y):\n",
        "    \"\"\"\n",
        "     Takes input as the processed data and labels, and trains a Random Forest Classifier model.\n",
        "    Args:\n",
        "        X: Processed data\n",
        "        y: Labels\n",
        "    Returns:\n",
        "        model: Trained model object\n",
        "    \"\"\"\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X, y)\n",
        "    print('Model trained.')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bagak30CGJpE"
      },
      "source": [
        "Model-1 train method "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EG9JOauuEe1_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_method1(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "     Takes train_file, val_file, and model_dir as input.\n",
        "     It trains on the train_file datapoints, and validates on the val_file datapoints.\n",
        "     While training and validating, it prints different evaluation metrics and losses, wherever necessary.\n",
        "     After finishing the training, it saves the best model in the model_dir.\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    val_df = pd.read_csv(val_file)\n",
        "    train_label = train_df['label']\n",
        "    val_label = val_df['label']\n",
        "    train_values, vectorizer = prepare_dataset(train_df, split='train')\n",
        "    val_values, _ = prepare_dataset(val_df, vectorizer=vectorizer, split='validation')\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model1(train_values, train_label)\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    val_pred = model.predict(val_values)\n",
        "    compute_performance(val_label, val_pred, split='validation')\n",
        "\n",
        "    # Save the model and vectorizer\n",
        "    model_file, vectorizer_file = save_model(model, vectorizer, model_dir)\n",
        "\n",
        "    return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i8EpxKJJDXW"
      },
      "source": [
        "Training model-1 with 25% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmw0kXrwJLch",
        "outputId": "45e45af8-9af1-4d46-c2fb-3a78fab04b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-1 using 25% of training data\n",
            "train data prepared with 3078 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7346\n",
            "validation F1-score (macro): 0.6614\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method1 function\n",
        "print('Train model-1 using 25% of training data')\n",
        "train_file = train_25_file  # path to your train_25_data file\n",
        "val_file = valid_file  # path to your valid_data file\n",
        "model_dir = MODEL_1_25_DIRECTORY  # path to the directory where you want to save the model\n",
        "\n",
        "model1_25_file, vectorizer1_25_file = train_method1(train_file, val_file, model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCMPyVBSJT_G"
      },
      "source": [
        "Training model-1 with 50% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im1EoQYeJZZm",
        "outputId": "d57043cd-b292-47d9-94c4-96353a84567d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-1 using 50% of training data\n",
            "train data prepared with 5386 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7422\n",
            "validation F1-score (macro): 0.6715\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method1 function\n",
        "print('Train model-1 using 50% of training data')\n",
        "train_file = train_50_file  #  path to your train_50_data file\n",
        "val_file = valid_file  # path to your valid_data file\n",
        "model_dir = MODEL_1_50_DIRECTORY  # path to the directory where you want to save the model\n",
        "\n",
        "model1_50_file, vectorizer1_50_file = train_method1(train_file, val_file, model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu6PsKI2Jjrf"
      },
      "source": [
        "Training model-1 with 75% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wtwlQjoJpl-",
        "outputId": "4dce0d27-9864-458d-9bed-878037072dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-1 using 75% of training data\n",
            "train data prepared with 7117 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7487\n",
            "validation F1-score (macro): 0.6814\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method1 function\n",
        "print('Train model-1 using 75% of training data')\n",
        "train_file = train_75_file  # path to your train_75_data file\n",
        "val_file = valid_file  # path to your valid_data file\n",
        "model_dir = MODEL_1_75_DIRECTORY  # path to the directory where you want to save the model\n",
        "\n",
        "model1_75_file, vectorizer1_75_file = train_method1(train_file, val_file, model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWWlret_JxHY"
      },
      "source": [
        "Training model-1 with 100% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIh2RoULEqap",
        "outputId": "f5435022-2216-434c-9fca-520f2d55cef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-1 using 100% of training data\n",
            "train data prepared with 12313 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7573\n",
            "validation F1-score (macro): 0.6953\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method2 function\n",
        "print('Train model-1 using 100% of training data')\n",
        "train_file = train_100_file  # path to your train_100_data file\n",
        "val_file = valid_file  # path to your valid_data file\n",
        "model_dir = MODEL_1_100_DIRECTORY  # path to the directory where you want to save the model\n",
        "\n",
        "model1_100_file, vectorizer1_100_file = train_method1(train_file, val_file, model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Fz9f7rGaPs"
      },
      "source": [
        "# Testing model-1 on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-CTWXoXNGuhp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def test_method1(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "    Take test_file, model_file, and output_dir as input.\n",
        "    Load the model and test examples from the test_file.\n",
        "    Print different evaluation metrics and save the output in the output directory.\n",
        "    \n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the test data from CSV file\n",
        "    test_df = pd.read_csv(test_file)\n",
        "    \n",
        "    # Extract the labels from the test data\n",
        "    test_label = test_df['label']\n",
        "\n",
        "    # Load the model and vectorizer\n",
        "    model, vectorizer = load_model(model_file, vectorizer_file) \n",
        "\n",
        "    # Prepare the test dataset for model prediction\n",
        "    test_values = vectorizer.transform(test_df['tweet'])\n",
        "\n",
        "    # Predict labels using the loaded model\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    # Add predicted labels to the test data\n",
        "    test_df['out_label']  = test_pred_label\n",
        "\n",
        "    # Compute performance metrics\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    # Save the model output to the output directory\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "    \n",
        "    return print(\"Predicted Test File is Uploaded and Five Examples of prediction are \\n\", test_df.iloc[0:6].head()) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxh4eFfkJ6sJ"
      },
      "source": [
        "Testing model-1 trained with 25% training dataset and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FagDGh-_KK4e",
        "outputId": "299bc245-e95d-48c1-b984-3ff1b45abed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 25% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7837\n",
            "test F1-score (macro): 0.6835\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/25/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 25% data')\n",
        "test_method1(test_file, model1_25_file, vectorizer1_25_file, MODEL_1_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ali9qvKHCu"
      },
      "source": [
        "Testing model-1 trained with 50% training dataset and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR2o7m8tKSul",
        "outputId": "fa63dae2-62f8-4c06-a08d-071b72d8a619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 50% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7744\n",
            "test F1-score (macro): 0.6736\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/50/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 50% data')\n",
        "test_method1(test_file, model1_50_file, vectorizer1_50_file, MODEL_1_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERNifOEJKICl"
      },
      "source": [
        "Testing model-1 trained with 75% training dataset and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rSP0f7JKbtD",
        "outputId": "0d76df24-378a-4c2d-9634-d55f6f8ff2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 75% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7814\n",
            "test F1-score (macro): 0.6916\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/75/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 75% data')\n",
        "test_method1(test_file, model1_75_file, vectorizer1_75_file, MODEL_1_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-DyE9BfKIoV"
      },
      "source": [
        "Testing model-1 trained with 100% training dataset and evaluating its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bop8dmA2G7AW",
        "outputId": "95eaf63a-6e33-4527-c5a5-045acb995cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 100% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7802\n",
            "test F1-score (macro): 0.6838\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/1/100/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       OFF\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method1(test_file, model1_100_file, vectorizer1_100_file, MODEL_1_100_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYjnwsjZG70q"
      },
      "source": [
        "# Model -2 (gradient boosting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6RfwAflHTIX"
      },
      "source": [
        "Model-2 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "izQjQEUuG-ZL"
      },
      "outputs": [],
      "source": [
        "def train_model2(X, y):\n",
        "    \"\"\"\n",
        "    Takes input as the processed data and labels, and trains a Gradient Boosting Classifier model.\n",
        "    Args:\n",
        "        X: Processed data\n",
        "        y: Labels\n",
        "    Returns:\n",
        "        model: Trained model object\n",
        "    \"\"\"\n",
        "    model = GradientBoostingClassifier()\n",
        "    model.fit(X, y)\n",
        "    print('Model trained.')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhIMKZhaHhtv"
      },
      "source": [
        "train method-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Ns_5lqN-HjBP"
      },
      "outputs": [],
      "source": [
        "def train_method2(train_file, val_file, model_dir):\n",
        "    \"\"\"\n",
        "     Takes train_file, val_file, and model_dir as input.\n",
        "     It trains on the train_file datapoints, and validates on the val_file datapoints.\n",
        "     While training and validating, it prints different evaluation metrics and losses, wherever necessary.\n",
        "     After finishing the training, it saves the best model in the model_dir.\n",
        "    Args:\n",
        "        train_file: Train file name\n",
        "        val_file: Validation file name\n",
        "        model_dir: Model output Directory\n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    val_df = pd.read_csv(val_file)\n",
        "    train_label = train_df['label']\n",
        "    val_label = val_df['label']\n",
        "    train_values, vectorizer = prepare_dataset(train_df, split='train')\n",
        "    val_values, _ = prepare_dataset(val_df, vectorizer=vectorizer, split='validation')\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model2(train_values, train_label)\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    val_pred = model.predict(val_values)\n",
        "    compute_performance(val_label, val_pred, split='validation')\n",
        "\n",
        "    # Save the model and vectorizer\n",
        "    model_file, vectorizer_file = save_model(model, vectorizer, model_dir)\n",
        "\n",
        "    return model_file, vectorizer_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OXSuZFuLMNi"
      },
      "source": [
        "Training model-2 with 25% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWkPYpMxLMNi",
        "outputId": "f3c5b8de-56ba-4d30-9a58-9fee2b91abf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-2 using 25% of training data\n",
            "train data prepared with 3078 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7346\n",
            "validation F1-score (macro): 0.6079\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method2 function\n",
        "print('Train model-2 using 25% of training data')\n",
        "train_file = train_25_file  # path to your train_25_data file\n",
        "val_file = valid_file  # path to your valid_data file\n",
        "model_dir = MODEL_2_25_DIRECTORY  # R path to the directory where you want to save the model\n",
        "\n",
        "model2_25_file, vectorizer2_25_file = train_method2(train_file, val_file, model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl3gDSADLMNi"
      },
      "source": [
        "Training model-2 with 50% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXUoRevlLMNi",
        "outputId": "b73843a7-986d-43d6-d98e-3cad201eca12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-2 using 50% of training data\n",
            "train data prepared with 5386 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7325\n",
            "validation F1-score (macro): 0.6003\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method2 function\n",
        "print('Train model-2 using 50% of training data')\n",
        "train_file = train_50_file  # path to your train_50_data file\n",
        "val_file = valid_file  # path to your valid_data file\n",
        "model_dir = MODEL_2_50_DIRECTORY  # path to the directory where you want to save the model\n",
        "\n",
        "model2_50_file, vectorizer2_50_file = train_method2(train_file, val_file, model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpAc5miHLMNj"
      },
      "source": [
        "Training model-2 with 75% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcVTFHhMLMNj",
        "outputId": "da0df15a-d018-476e-e423-e488f9954f92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-2 using 75% of training data\n",
            "train data prepared with 7117 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7346\n",
            "validation F1-score (macro): 0.6036\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method2 function\n",
        "print('Train model-2 using 75% of training data')\n",
        "train_file = train_75_file  # path to your train_75_data file\n",
        "val_file = valid_file  #  path to your valid_data file\n",
        "model_dir = MODEL_2_75_DIRECTORY  # path to the directory where you want to save the model\n",
        "\n",
        "model2_75_file, vectorizer2_75_file = train_method2(train_file, val_file, model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Jd3PgPLMNj"
      },
      "source": [
        "Training model-2 with 100% training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuEkiL5YHvDd",
        "outputId": "9c5c8bc8-6ff5-421c-c377-1ecc5f692908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train model-2 using 100% of training data\n",
            "train data prepared with 12313 datapoints.\n",
            "validation data prepared with 927 datapoints.\n",
            "Model trained.\n",
            "validation Accuracy: 0.7400\n",
            "validation F1-score (macro): 0.6179\n",
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Call the train_method2 function\n",
        "print('Train model-2 using 100% of training data')\n",
        "train_file = train_100_file  # path to your train_100_data file\n",
        "val_file = valid_file  # path to your valid_data file\n",
        "model_dir = MODEL_2_100_DIRECTORY  # path to the directory where you want to save the model\n",
        "\n",
        "model2_100_file, vectorizer2_100_file = train_method2(train_file, val_file, model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLG2EfIqHkEL"
      },
      "source": [
        "## Testing method -2 on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5FMehCBPHstz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def test_method2(test_file, model_file, vectorizer_file, output_dir):\n",
        "    \"\"\"\n",
        "    Take test_file, model_file, and output_dir as input.\n",
        "    Load the model and test examples from the test_file.\n",
        "    Print different evaluation metrics and save the output in the output directory.\n",
        "    \n",
        "    Args:\n",
        "        test_file: Test file name\n",
        "        model_file: Model file name\n",
        "        vectorizer_file: Vectorizer file name\n",
        "        output_dir: Output Directory\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the test data from CSV file\n",
        "    test_df = pd.read_csv(test_file)\n",
        "    \n",
        "    # Extract the labels from the test data\n",
        "    test_label = test_df['label']\n",
        "\n",
        "    # Load the model and vectorizer\n",
        "    model, vectorizer = load_model(model_file, vectorizer_file) \n",
        "\n",
        "    # Prepare the test dataset for model prediction\n",
        "    test_values = vectorizer.transform(test_df['tweet'])\n",
        "\n",
        "    # Predict labels using the loaded model\n",
        "    test_pred_label = model.predict(test_values)\n",
        "\n",
        "    # Add predicted labels to the test data\n",
        "    test_df['out_label']  = test_pred_label\n",
        "\n",
        "    # Compute performance metrics\n",
        "    test_f1_score = compute_performance(test_label, test_pred_label, split='test')\n",
        "\n",
        "    # Save the model output to the output directory\n",
        "    out_file = os.path.join(output_dir, 'output_test.csv')\n",
        "    print('Saving model output to', out_file)\n",
        "    test_df.to_csv(out_file)\n",
        "    \n",
        "    return print(\"Predicted Test File is Uploaded and Five Examples of prediction are \\n\", test_df.iloc[0:6].head()) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyHrUgIfMK_1"
      },
      "source": [
        "Testing model-2 trained with 25% training dataset and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ws44moMMK_1",
        "outputId": "79e4d680-5f38-4e36-83bc-412cb6c72b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 25% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7767\n",
            "test F1-score (macro): 0.6184\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/25/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 25% data')\n",
        "test_method2(test_file, model2_25_file, vectorizer2_25_file, MODEL_2_25_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-LLDC4GMK_2"
      },
      "source": [
        "Testing model-2 trained with 50% training dataset and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLjAy5mJMK_2",
        "outputId": "6d25e7fa-6a9f-4228-ad72-795c115e48ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 50% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7814\n",
            "test F1-score (macro): 0.6356\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/50/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       OFF\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 50% data')\n",
        "test_method2(test_file, model2_50_file, vectorizer2_50_file, MODEL_2_50_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRhO_Je6MK_3"
      },
      "source": [
        "Testing model-2 trained with 75% training dataset and evaluating its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2G3KARhMK_3",
        "outputId": "8870b5e9-ebfa-4878-ea16-6f7f59e597c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 75% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7779\n",
            "test F1-score (macro): 0.6174\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/75/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       NOT\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 75% data')\n",
        "test_method2(test_file, model2_75_file, vectorizer2_75_file, MODEL_2_75_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzSDWRtMK_4"
      },
      "source": [
        "Testing model-2 trained with 100% training dataset and evaluating its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvk_VmKoMK_4",
        "outputId": "255c49fb-bbcc-45bc-8366-4d3decd4b26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing using model trained on 100% data\n",
            "Model and vectorizer loaded successfully.\n",
            "test Accuracy: 0.7767\n",
            "test F1-score (macro): 0.6184\n",
            "Saving model output to gdrive/MyDrive/./CE807/Assignment2/2201404/models/2/100/output_test.csv\n",
            "Predicted Test File is Uploaded and Five Examples of prediction are \n",
            "       id                                              tweet label out_label\n",
            "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   OFF       NOT\n",
            "1  27014  #ConstitutionDay is revered by Conservatives, ...   NOT       NOT\n",
            "2  30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   NOT       NOT\n",
            "3  13876  #Watching #Boomer getting the news that she is...   NOT       NOT\n",
            "4  60133  #NoPasaran: Unity demo to oppose the far-right...   OFF       NOT\n"
          ]
        }
      ],
      "source": [
        "print('Testing using model trained on 100% data')\n",
        "test_method2(test_file, model2_100_file, vectorizer2_100_file, MODEL_2_100_DIRECTORY)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
